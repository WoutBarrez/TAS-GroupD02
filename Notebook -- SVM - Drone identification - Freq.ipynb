{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Sample data representing air pressure readings for each drone\n",
    "# Each row represents a drone, and each column represents a feature (e.g., microphone array readings)\n",
    "data_Altura = []\n",
    "data_ANWB = []\n",
    "data_ATMOS = []\n",
    "data_Autel_Evo = []\n",
    "data_Phantom = []\n",
    "name_dic = {'Altura.csv': data_Altura, 'ANWB.csv': data_ANWB, 'ATMOS.csv': data_ATMOS, 'Autel_Evo.csv': data_Autel_Evo, 'Phantom.csv': data_Phantom}\n",
    "\n",
    "def datasets():\n",
    "    data_lst = ['Altura.csv', 'ANWB.csv', 'ATMOS.csv', 'Autel_Evo.csv', 'Phantom.csv']\n",
    "    for filename in data_lst:\n",
    "        data = []\n",
    "        with open(filename, \"r\") as file:\n",
    "            csvreader = csv.reader(file)\n",
    "            for row in csvreader:\n",
    "                row1 = []\n",
    "                for i,e in enumerate(row):\n",
    "                    row1.append(float(e))\n",
    "                data.append(row1)\n",
    "        step = np.array(data[:249999])\n",
    "        for i,row in enumerate(step.T):\n",
    "            if i != 0 and i != 16 and  i !=20 and i != 40 and  i !=62 and  i !=63:\n",
    "                name_dic[filename].append(row)\n",
    "\n",
    "datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining SVM classifier (extracted form AI course)\n",
    "\n",
    "# Set the PyTorch and numpy random seeds for reproducibility:\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "class MLPClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_hidden_neurons, n_classes, learning_rate, n_epochs):\n",
    "        \"\"\"\n",
    "        Initialize the neural network classifier \n",
    "        \"\"\"\n",
    "        # initialize superclass\n",
    "        super(MLPClassifier,self).__init__()\n",
    "\n",
    "        # the number of classes\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # the number of epochs\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        # create the neural network\n",
    "        self.network = torch.nn.Sequential(torch.nn.Linear(n_features, n_hidden_neurons),\n",
    "                            torch.nn.ReLU(),torch.nn.Linear(n_hidden_neurons,n_hidden_neurons),torch.nn.ReLU(),torch.nn.Linear(n_hidden_neurons,n_classes))\n",
    "\n",
    "\n",
    "\n",
    "        # the cross-entropy loss function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # the Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network\n",
    "        \"\"\"\n",
    "        return self.network(X)\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the neural network classifier \n",
    "        \"\"\"\n",
    "\n",
    "        # convert data into appropriate format for {torch}\n",
    "        X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_torch = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=self.n_classes).to(dtype=torch.float32)\n",
    "\n",
    "        # start training\n",
    "        print(\"Training started...\")\n",
    "        for epoch in range(0, self.n_epochs):\n",
    "            # Set optimizer to zero grad\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward step\n",
    "            y_pred = self.forward(X_train_torch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss_fn(y_pred, y_train_torch)\n",
    "\n",
    "            # Backward step\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Report loss\n",
    "            if not ((epoch+1) % 100):\n",
    "                print(\"Epoch:\", epoch+1, f\", loss = {loss.detach().item():.4f} \")\n",
    "        print(f\"Training completed in {epoch+1} epochs! Final loss = {loss.detach().item():.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Use the trained neural network to predict the labels of the test set \n",
    "        \"\"\"\n",
    "        # convert data into appropriate format for {torch}\n",
    "        X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "        arg = self.forward(X_test_torch)\n",
    "        return torch.argmax(arg,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch: 100 , loss = 0.0486 \n",
      "Epoch: 200 , loss = 0.0105 \n",
      "Epoch: 300 , loss = 0.0042 \n",
      "Epoch: 400 , loss = 0.0018 \n",
      "Epoch: 500 , loss = 0.0008 \n",
      "Epoch: 600 , loss = 0.0004 \n",
      "Epoch: 700 , loss = 0.0003 \n",
      "Epoch: 800 , loss = 0.0002 \n",
      "Epoch: 900 , loss = 0.0001 \n",
      "Epoch: 1000 , loss = 0.0001 \n",
      "Training completed in 1000 epochs! Final loss = 0.0001\n",
      "tensor([4, 2, 3,  ..., 3, 2, 0])\n",
      "Accuracy: 0.9360344827586207\n"
     ]
    }
   ],
   "source": [
    "# Slice the data into a specified number (percentage - sample_percent) of separate datasets\n",
    "# e.g. 4 training datasets and 1 test data set, or 3 training data sets and 2 test data sets\n",
    "def data_clean(drone, sample_percent):\n",
    "    X_data = []\n",
    "    data_arr = np.array(name_dic[drone]).T\n",
    "    total_time_points_no = len(data_arr)\n",
    "    sample_size = int(total_time_points_no * sample_percent)\n",
    "    for i in range(0,int(1/sample_percent)):\n",
    "        X = data_arr[sample_size*i:sample_size*(i+1)].T\n",
    "        X_data  += [X]\n",
    "    return np.array(X_data)\n",
    "\n",
    "sample_percent = 0.01\n",
    "X_data_Altura = data_clean('Altura.csv', sample_percent)\n",
    "X_data_ANWB = data_clean('ANWB.csv', sample_percent)\n",
    "X_data_ATMOS = data_clean('ATMOS.csv', sample_percent)\n",
    "X_data_Autel_Evo = data_clean('Autel_Evo.csv', sample_percent)\n",
    "X_data_Phantom = data_clean('Phantom.csv', sample_percent)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in X_data_Altura:\n",
    "    for e in i:\n",
    "        X_data += [e]\n",
    "        y_data += [0]\n",
    "for i in X_data_ANWB:\n",
    "    for e in i:\n",
    "        X_data += [e]\n",
    "        y_data += [1]\n",
    "for i in X_data_ATMOS:\n",
    "    for e in i:\n",
    "        X_data += [e]\n",
    "        y_data += [2]\n",
    "for i in X_data_Autel_Evo:\n",
    "    for e in i:\n",
    "        X_data += [e]\n",
    "        y_data += [3]\n",
    "for i in X_data_Phantom:\n",
    "    for e in i:\n",
    "        X_data += [e]\n",
    "        y_data += [4]\n",
    "train, test, train_labels, test_labels = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "n_features = 2499\n",
    "n_hidden_neurons = 50 #120 \n",
    "n_classes = 5\n",
    "learning_rate = 0.02 #0.008\n",
    "n_epochs = 1000 #3000\n",
    "clf = MLPClassifier(n_features, n_hidden_neurons, n_classes, learning_rate, n_epochs)\n",
    "\n",
    "# Train the classifier\n",
    "clf.train(train,train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predict = clf.predict(test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, y_predict, normalize=True)\n",
    "\n",
    "print(\"Prediction accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "#Input selected microphone recording to determine drone\n",
    "test_data = clf.predict([X_data_Phantom[80][9]])\n",
    "print(test_data) #make formatting/text that corresponds with the predicted drone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
